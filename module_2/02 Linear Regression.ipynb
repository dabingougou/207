{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHLcriKWLRe4"
   },
   "source": [
    "# Assignement 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\"> Submission requirements </span>\n",
    "\n",
    "Your homework will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n",
    "\n",
    "Additional points may be deducted if these requirements are not met:\n",
    "\n",
    "    \n",
    "* Comment your code;\n",
    "* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own;\n",
    "* Try and minimize the use of the global namespace (meaning, keep things inside functions).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X58hOMTUH-w"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1d_data(num_examples, w, b, bound):\n",
    "  \"\"\"Create X, Y data with a linear relationship with added noise.\n",
    "\n",
    "  Args:\n",
    "    num_examples: number of examples to generate\n",
    "    w: desired slope\n",
    "    b: desired intercept\n",
    "    bound: lower and upper boundary of the data interval\n",
    "\n",
    "  Returns:\n",
    "    X and Y with shape (num_examples)\n",
    "  \"\"\"\n",
    "  np.random.seed(4)  # consistent random number generation\n",
    "  X = np.arange(num_examples)\n",
    "  deltas = np.random.uniform(low=-bound, high=bound, size=X.shape) # added noise\n",
    "  Y = b + deltas + w * X\n",
    "\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating some artificial data using the <span style=\"color:chocolate\">create_1d_data()</span> function defined at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qJg0IiYVJ8U"
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_examples = 70\n",
    "w = 2\n",
    "b = 1\n",
    "bound = 1\n",
    "\n",
    "# Create data\n",
    "X, Y = create_1d_data(num_examples, w, b, bound)\n",
    "\n",
    "# Print shapes\n",
    "print('Printing shape of X:', X.shape)\n",
    "print('Printing first 10 elements in X:', X[:10])\n",
    "print('\\nPrinting shape of Y:', Y.shape)\n",
    "print('Printing first 10 elements in Y:', Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NXo1n9j1LMT"
   },
   "source": [
    "---\n",
    "### Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objectives here involve adding one more feature to X and creating data partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 1:</span> Adding features (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add a column of $1s$ to $X$ (this will serve as an intercept or \"bias\" in our modeling task later on). Note: do not create a new array, just concatenate with the current values;\n",
    "2. Print the shape of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 2:</span> Data splits (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <span style=\"color:chocolate\">train_test_split()</span> method available in scikit-learn:\n",
    "1. Split the (X,Y) data into training and test paritions by setting test_size=0.2 and random_state=1234. All the other arguments of the method are set to default values. Name the resulting arrays X_train, X_test, Y_train, Y_test;\n",
    "2. Print the shape of each array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 3:</span> Plots (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a side-by-side histogram for the values in Y_train and X_train. Make sure to include axes name and titles for each subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by considering the two models proposed in Assignment 1:\n",
    "1. $M_1(x) = 5+x$ \n",
    "2. $M_2(x) = 1+2x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 4:</span> Practice with Parameters (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following computations:\n",
    "\n",
    "1. Use matrix multiplication (np.dot) to create $M_1$ and $M_2$ (as previously defined) to produce vectors of predictions using the X_train data. Call these predictions M1_hat_train, M2_hat_train. Hint: the \"learned\" parameters are alredy provided to you;\n",
    "3. Print the shapes of the predictions to verify that they match the shape of Y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBEZ_QOX6qOi"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 5:</span> Learn parameters with Gradient Descent (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaXYiTm9ftRf"
   },
   "source": [
    "1. Fill in the <span style=\"color:green\">NotImplemented</span> parts of the <span style=\"color:chocolate\">gradient_descent()</span> function below. Hint: refer to ``02 Linear Regression_helper.ipynb file``;\n",
    "\n",
    "2. Run this function with our artificial (X_train, Y_train) data . Set learning_rate = .0002 and num_epochs = 5. Print out the weights and loss after each epoch. \n",
    "\n",
    "3. Generate a plot with the loss values on the y-axis and the epoch number on the x-axis for visualization. Make sure to include axes name and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hP9rzDyFXTg"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(inputs, outputs, learning_rate, num_epochs):\n",
    "  \"\"\"Apply the gradient descent algorithm to learn learn linear regression.\n",
    "\n",
    "  Args:\n",
    "    inputs: A 2-D array where each column is an input feature and each\n",
    "            row is a training example.\n",
    "    outputs: A 1-D array containing the real-valued\n",
    "             label corresponding to the input data in the same row.\n",
    "    learning_rate: The learning rate to use for updates.\n",
    "    num_epochs: The number of passes through the full training data.\n",
    "\n",
    "  Returns:\n",
    "    weights: A 2-D array with the learned weights after each training epoch.\n",
    "    losses: A 1-D array with the loss after each epoch.\n",
    "  \"\"\"\n",
    "  # m = number of examples, n = number of features\n",
    "  m, n = inputs.shape\n",
    "  \n",
    "  # We'll use a vector of size n to store the learned weights and initialize\n",
    "  # all weights to 1. \n",
    "  W = np.ones(n)\n",
    "  \n",
    "  # Keep track of the training loss and weights after each step.\n",
    "  losses = []\n",
    "  weights = []\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    # Append the old weights to the weights list to keep track of them.\n",
    "    weights.append(W)\n",
    "\n",
    "    # Evaluate the current predictions for the training examples given\n",
    "    # the current estimate of W. \n",
    "    predictions = NotImplemented\n",
    "    \n",
    "    # Find the difference between the predictions and the actual target\n",
    "    # values.\n",
    "    diff = NotImplemented\n",
    "    \n",
    "    # In standard linear regression, we want to minimize the sum of squared\n",
    "    # differences. Compute the mean squared error loss. Don't bother with the\n",
    "    # 1/2 scaling factor here.\n",
    "    loss = NotImplemented\n",
    "\n",
    "    # Append the loss to the losses list to keep a track of it.\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Compute the gradient with respect to the loss.\n",
    "    # [Formula (4) in the Gradient Descent Implementation]\n",
    "    gradient = NotImplemented\n",
    "\n",
    "    # Update weights, scaling the gradient by the learning rate.\n",
    "    W = W - learning_rate * gradient\n",
    "      \n",
    "  return np.array(weights), np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning plays an important role in optimizing ML models. One systematically adjusts hyperparameters to find the right balance between model complexity and generalization, ultimately leading to better predictive performance and model effectiveness.\n",
    "\n",
    "Note that hyperparameter tuning is typically performed on **a separate validation dataset**. However, for the purposes of this assignment and based on the ML knowledge you've acquired thus far, we will perform hyperparameter tuning directly on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 6:</span> Tuning hyperparameters (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "    \n",
    "1. Do you think it would be beneficial to extend the model training in Exercise 5 by increasing the learning rate to 0.02? Justify your answer;\n",
    "\n",
    "2. Restoring to the previous learning_rate = .0002, would you  consider extending the model training in Exercise 5 by increasing the number of epochs to 10? Justify your answer.\n",
    "\n",
    "Note: to support your answers, we recommend the following actions:\n",
    "- create side-by-side subplots to show the loss at each epoch (make sure to include axes name and title), and\n",
    "- print the loss at the last epoch under the following scenarios:\n",
    "    - learning_rate=.0002, num_epochs=5;\n",
    "    - learning_rate=.02, num_epochs=5;\n",
    "    - learning_rate=.0002, num_epochs=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 7:</span> Choosing the best model (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the learned model that you consider most optimal given your answers to the hyperparameter tuning exercise. Call this model $M_3(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Evaluation and Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our models, we assess their performance based on how closely they fit the available data. In other words, we compare the true value $y$ with the predicted value $\\hat{y}$ for each $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 8:</span> Computing MSE (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the three models ($M_1(x)$, $M_2(x)$, $M_3(x)$) by computing the MSE metric on the training dataset. Hint: you can use the <span style=\"color:chocolate\">mean_squared_error()</span> method available in sklearn.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 9:</span> Generalization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the three models ($M_1(x)$, $M_2(x)$, $M_3(x)$) demonstrates better generalization ability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### <span style=\"color:chocolate\">Additional practice question</span> (not graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does stochastic gradient descent (SGD) differ from the gradient descent (GD) approach implemented in Exercise 5?\n",
    "2. Determine the optimal weights for model $M_3(x)$ using SGD. Does SGD converge more rapidly? Explain the reasons for your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02 Linear Regression.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
